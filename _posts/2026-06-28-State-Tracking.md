---
layout: distill
title: State Tracking with Sequence Models
description: solving word problems
tags: state-tracking generalization
giscus_comments: true
date: 2025-06-29
featured: true
mermaid:
  enabled: true
  zoomable: true
code_diff: true
map: true
chart:
  chartjs: true
  echarts: true
  vega_lite: true
tikzjax: true
typograms: true

bibliography: 2025-06-28-State-Tracking.bib

---



Large Language Models can perform impressive tasks such as storytelling, instruction following, and code generation. These capabilities seem to suggest an ability to maintain a complex internal "state," keeping track of context and dependencies over long sequences. And yet, their performance often falters on precise compositional tasks, such as logical puzzles and arithmetic, which involve keeping track of intermediate results <d-cite key="dziri2023faithfatelimitstransformers"></d-cite>.

Most LLMs are based on the transformer architecture, as it performs well on language modeling. However, under a standard conjecture in computational complexity theory, transformers may not be well suited to the state tracking task.

We'll define state tracking, introduce relevant concepts from circuit complexity, analyze how Transformers and RNNs perform, discuss limitations, and explore potential solutions.

## What is State Tracking? 
Formally, we can consider state tracking as simulating the transition function of a semiautomaton, following <d-cite key="liu2023transformerslearnshortcutsautomata"></d-cite>:

Let $$\mathcal{A} = (Q,\Sigma, \delta)$$ be a *semiautomaton*, which consists of a finite set of states $$Q$$, an input alphabet $$\Sigma$$, and a transition function $$\delta : Q \times \Sigma \rightarrow Q.$$ We can think of $$\sigma \in \Sigma$$ as actions or transformations that update the state from $$q$$ to $$q^\prime=\delta(q, \sigma)$$. Given a starting state $$q_0$$ and an input sequence $$\sigma_{1:T} = (\sigma_1,\sigma_2,...,\sigma_T)$$ we get a sequence $$(q_1,...,q_T)$$ by the action $$q_t=\delta (q_{t-1},\sigma_t)$$ for $$t=1,...,T.$$ This defines a mapping $$\mathcal{A}_{T,q_0}: \Sigma^T \rightarrow Q^T$$ by $$\mathcal{A}_{T,q_0} (\sigma_1,\sigma_2,...,\sigma_T) = (q_1,...,q_T).$$

We say that a function $$f:\Sigma^T \rightarrow Q^T$$ *simulates* $$\mathcal{A}$$ if for all input sequences $$\sigma_{1:T}$$, $$f(\sigma_{1:T}) = \mathcal{A}_{T,q_0}(\sigma_{1:T})$$. Usually, $$f$$ is chosen to be a sequence model like a Transformer or RNN.

To simplify our discussion, we will define the *state tracking problem* as the task of finding the final state, given the initial state $$q_0$$ and an input sequence $$(\sigma_1,\sigma_2,...,\sigma_T)$$. We focus on this final state computation to understand the limits of models tackling this problem.

## Introduction to Circuit Complexity

To analyze these limits, we can use the framework of *circuit complexity*, which measures the resources required to compute a function using a family of boolean circuits, denoted $$\{C_n\}^\infty _{n=0}.$$ This framework usually considers functions with output in $$\{ 0, 1\}$$, so computations need to be reformulated into a decision problem. In the context of our state tracking problem, for input size $$T$$, we want a function $$f$$ such that $$f(\sigma_1,\sigma_2,...,\sigma_T) =1$$ if $$\pi_T\left(\mathcal{A}_{T,q_0}(\sigma_1,\sigma_2,...,\sigma_T) \right)=q_T$$ and $$0$$ otherwise.

**Definition:** A *boolean circuit* $$C_n$$ is a function $$\{0,1\}^n \rightarrow \{0,1\}$$, represented by a directed acyclic graph with 3 types of nodes:

- **Input nodes:** $$n$$ nodes with an in-degree of $$0$$, corresponding to the $$n$$ input bits.
- **Gate nodes:** Nodes with positive in-degree and out-degree, representing intermediate computations. Each is labeled with a boolean gate (e.g., AND, OR, NOT, MAJORITY).
- **Output node:** A single node with an out-degree of 0, representing the circuit's output. 

We analyze a family of boolean circuits $$\{C_n\}_{n=0}^\infty$$ to describe how computational resources scale with the input size $$n$$. The number of nodes in $$C_n$$ is its **size**, $$s(n)$$, and the length of the longest path from an input to the output is its **depth**, $$d(n)$$.

**Definition.** Let $$\Sigma$$ be a non-empty set, called an alphabet. Consider the free monoid construction, $$\Sigma^*=\cup _{i\geq 0} \Sigma^i$$ where $$\Sigma^i$$ denotes i-fold concatenation and $$\Sigma^0 = \{ \epsilon \}$$ is the set containing the empty string.  $$\Sigma^*$$ is the set of strings of finite length generated by the alphabet $$\Sigma,$$ including the empty string $$\epsilon$$. The operator $$^*$$ is called the Kleene star operator. A *formal language* is a subset $$L \subseteq \Sigma^*$$. We are interested in binary languages, $$L \subseteq \{0,1\}^*$$ and in particular its *characteristic function* $$f_L: \{0,1\}^* \rightarrow \{0,1\},$$ where $$f_L(x) =1$$ if and only if $$x \in L.$$ 

**Example:** Consider the language $$L_{PARITY}=(11)^*=\{\epsilon, 11,1111,....\}$$ which contains all strings of $$1$$'s of even length. The characteristic function is $$f(1^k)=1$$ if $$k$$ is even, and  $$f(1^k)=0$$ if $$k$$ is odd. 

**Definition:** A family of boolean circuits $$\{C_n\}_{n=0}^\infty$$ *recognizes a language* $$L$$ if for any $$n\geq 0$$ and string $$x\in\{0,1\}^n,$$ $$x\in L$$ if and only if $$C_n(x)=1.$$ We can also formulate this using the characteristic function: for any $$x\in \{0,1\}^n,$$ $$f_L(x)=C_n(x)$$.



**Definition:** A function $$f: \{0,1\}^* \rightarrow \{0,1\}$$  is in $$TC^0$$ if there exists a polynomial $$p$$, a constant $$d$$, and a family of boolean circuits $$\{C_n\}_{n=0}^\infty$$ such that:

- each circuit is made of AND, OR, NOT and MAJORITY gates of unbounded fan in,
- each $$C_n$$ is of size $$p(n)$$,
- has constant depth $$d$$
- for an input of size $$n$$, $$x \in \{0,1\}^n$$, we have $$f(x)=C_n(x)$$. 

**Definition:** A function $$f: \{0,1\}^* \rightarrow \{0,1\}$$ is in $$NC^1$$ if there exists a polynomial $$p$$, a constant $$d$$, and a family of boolean circuits $$\{C_n\}_{n=0}^\infty$$ such that:

- each circuit is made of AND, OR, NOT gates of fan in 2,
- each $$C_n$$ is of size $$p(n)$$,
- $$C_n$$ has logarithmic depth $$O(\log(n))$$,
- for an input of size $$n$$, $$x \in \{0,1\}^n$$, we have $$f(x)=C_n(x)$$. 

**Definition:** a language $$L$$ is in $$TC^0$$ (or $$NC^1$$) if its characteristic function $$f_L$$ is in the corresponding class.

The definitions above can be extended to functions with multiple output bits: let $$in(n), out(n)$$ be non-negative, non-decreasing, polynomially bounded functions, and consider boolean circuits $$C_n:\{0,1\}^{in(n)} \rightarrow \{0,1\}^{out(n)}$$. For example, $$n$$-bit addition is a function $$f:\{0,1\}^{2n}\rightarrow \{0,1\}^{n+1}$$ <d-cite key="doi:10.1137/0215070"></d-cite>.

Although state tracking and related problems of interest will be represented with binary valued functions, some intermediate computations will involve multiple bit outputs. The main point is that some functions can be computed with constant depth circuits (in $$TC^0$$), while others need logarithmic depth (in $$NC^1$$). 
**Examples:** 
- $$n$$ bit addition, multiplication and division is in $$TC^0$$ 
- $$L_{PARITY}$$ is in $$TC^0$$
- A computation with fixed sized input is vacuously in $$TC^0$$. $$k \times k$$ matrix multiplication for fixed $$k$$ is in $$TC^0$$.

A standard fact in circuit complexity theory is that $$TC^0 \subseteq NC^1$$. It's an open problem whether the containment is strict, although it is believed that $$TC^0 \neq NC^1$$.

## State Tracking and Circuit Complexity
Note that the state tracking problem can be solved by a circuit in $$NC^1$$ by formulating it as a matrix multiplication problem. For each state $$q\in Q$$, we can encode it into a $$|Q|$$-dimensional one-hot vector. Each input symbol $$\sigma \in \Sigma$$ gives us a map $$\delta(-, \sigma):Q \rightarrow Q$$ which we can think of as a matrix $$M_\sigma \in \{0,1\}^{|Q| \times |Q| }$$. We have $$[M_\sigma]_{i,j}=1_{\{ \delta(e_j , \sigma) = e_i \}}$$ and $$M_\sigma$$ acts on state vectors by left multiplication. This matrix multiplication can be performed by a constant depth circuit, and we can apply a divide and conquer approach to perform all the multiplications in a $$O(\log T)$$ depth circuit. Therefore, state tracking is in $$NC^1$$.


Using log precision, Transformers are in $$TC^0$$ <d-cite key="merrill2023parallelismtradeofflimitationslogprecision"></d-cite>. If we believe $$TC^0 \neq NC^1$$, then a constant depth transformer would not be able to solve the state tracking problem for arbitrary sequence length. This could explain why transformers fail to generalize to longer sequences and harder problems. If we allow variable depth based on input size, we can construct a transformer with $$O(\log T)$$ layers to solve the state tracking problem of length $$T$$ <d-cite key="liu2023transformerslearnshortcutsautomata"></d-cite>. While a transformer performing next token prediction may not be as expressive as an RNN, allowing it to output intermediate tokens can increase expressivity. A transformer model performing chain of thought for $$O(n)$$ intermediate steps <d-cite key="merrill2024expressivepowertransformerschain"></d-cite> can simulate $$NC^1$$ circuits (see also <d-cite key="li2024chainthoughtempowerstransformers"></d-cite>). 

Recurrent Neural Networks (RNNs) can solve state tracking in a single layer. An RNN maps a sequence $$(x_1,...,x_T)$$ to $$(y_1,...y_T)$$ and is given by the recurrence
$$h_{t+1}=g(h_t, x_t), \ y_t=dec(h_t,x_t)$$
where $$g$$ and $$dec$$ are learnable, nonlinear functions. Let $$x_t=\sigma_t$$ and $$h_t =q_{t-1}$$ be the 1-hot vector representing the state. $$g(-,x_t)$$ can simulate the state transition function $$\delta (- ,\sigma_t)$$ (<d-cite key="merrill2025illusionstatestatespacemodels"></d-cite>, theorem 5.1). In particular, even if we constrain $$g(-,x_t)$$ to be a linear transformation, it can represent the state transition matrix $$M_{\sigma_t}$$, and so linear RNNs with general state matrix can also solve the state tracking problem.

Although state tracking and other relevant problems lie in $$NC^1$$, we will focus on a specific synthetic task, to which any other task in $$NC^1$$ can be reduced.

### The Word Problem: A Proxy for $$NC^1$$

*Word problem for a finite group* $$G:$$ given a sequence $$(x_1,x_2,...x_n) \in G^n$$ and $$y \in G$$, determine if $$x_1 x_2...x_n = y$$. It is equivalently formulated as: given a sequence $$(g_1,...,g_n) \in G^n$$, determine if $$g_1g_2,...,g_n = e$$. 

- For solvable groups, like $$A_4$$ and $$\mathbb{Z}_{60}$$, this is in $$TC^0$$ (via Khoan Rhoades theorem). 
- For nonsolvable groups (such as $$A_5$$ and $$S_5$$), it is $$NC^1$$ complete.

Barrington's theorem <d-cite key="BARRINGTON1989150"></d-cite> states that any circuit in $$NC^1$$, with input size $$n$$, can be reduced to a circuit calculating the word problem on $$A_5$$ of polynomial length in $$n$$. (Thms 1 and 5). This means solving the $$A_5$$​ word problem is, in a sense, as hard as any other problem in $$NC^1$$, including state tracking. We will therefore use the $$A_5$$​ word problem as a benchmark for evaluating model capabilities.



### No. of Layers vs Sequence Length

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="lazy" path="assets/img/2025-07-04-State-Tracking/graphs_3tasks.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    This is a caption for my PNG image.
</div>

Now let's show some empirical results showing how transformers do on this problem. For $$G \in \{\mathbb{Z}_{60}, A_4 \times \mathbb{Z}_5, A_5 \}$$, we train llama based transformers to acheive >90% validation accuracy, following the experiment from <d-cite key="merrill2025illusionstatestatespacemodels"></d-cite>. We directly encode group elements into tokens instead of tokenizing its string based representation.

Here, the standard task is: fix an maximum sequence length $$n$$. For $$k\leq n$$, given an input sequence $$(x_1,...,x_k)\in G^k$$, we want our model to calculate $$f_\theta(x_1,...,x_k)=y_k$$ where $$y_k=x_1 \cdot ...\cdot x_k$$. 

The chain of thought task is: $$f_\theta (x_1,...,x_n, c, y_1,...,y_k)=y_{k+1}$$ where $$c$$ is a placeholder token. For $$k=0$$ we have $$f_\theta (x_1,...,x_n, c)=y_{1}$$. 
In particular, in <d-cite key="li2024chainthoughtempowerstransformers"></d-cite> they construct a 2 layer transformer that can solve the word problem on $$S_5$$.

A similar idea to CoT is pursued in <d-cite key="liu2023transformerslearnshortcutsautomata"></d-cite>, called "scratchpad training" where the output tokens are interleaved with the input tokens $$(x_1,y_1,x_2,y_2,...,x_n,y_n)$$ and the model is trained autoregressively. Intuitively, these tasks are easier because the model can look at its intermediate results to perform the next calculation, avoiding the need for logarithmic depth.

In the figure, we see that for $$A_5$$, the transformer model needs more layers to handle longer sequences, while the LSTM model can predict with just one layer. The transformer model in Chain of Thought mode also only needs 1 layer to solve this task. Interestingly, the word problem for $A_4\times mathbb{Z}_5$ is almost as hard as the $A_5$ word problem.


## Linear RNNs

Linear Recurrent Neural Networks (also called state space models) are motivated by the goal of developing fast, efficient sequence models capable of performance comparable to Transformers, particularly in language modeling. Unlike a traditional RNN, the hidden state is updated through an affine transformation. The condition that it's affine allows us to use parallel associative scan for the forward pass. We'll follow notation from  <d-cite key="grazzi2025unlockingstatetrackinglinearrnns"></d-cite>:

**Definition:** A linear RNN layer is a function $$f_\theta : \mathbb{R}^l \times \mathbb{C}^{n\times d} \rightarrow   \mathbb{R}^l \times \mathbb{C}^{n\times d}$$ that maps an input, hidden state pair $$(x_t, H_{t-1})$$ to output, next hidden state pair $$(\hat y_t, H_{t})$$ in the following way:

$$
\begin{gathered}
H_t = A(x_t)H_{t-1}+B(x_t),\ \hat y_t = dec(H_t,x_t)\ \text{for }  t\in \{ 1,...,T\} \\
H_0 \in\mathbb{C}^{n\times d},\ A:\mathbb{R}^l \rightarrow \mathbb{C}^{n\times n},\ B:\mathbb{R}^l \rightarrow \mathbb{C}^{n\times d},\ dec: \mathbb{C}^{n\times d} \times\mathbb{R}^l \rightarrow \mathbb{R}^p
\end{gathered}
$$

where $$A,B$$ and $$dec$$ are learnable, and generally nonlinear functions of the network parameters $$\theta$$. We can stack multiple layers by feeding the outputs of one layer as the inputs to the next layer. For the rest of this post we will focus on real valued $$A$$ and $$B$$.


The structure of the state transition matrix function $$A$$ determines the expressivity of the linear RNN. If we allow $$A$$ to be a general real valued matrix, we can simulate a semiautomaton by setting $$A$$ to be the state transition matrix.  However, we constrain $$A$$ in various ways in order to train them efficiently. For stable training, we bound the eigenvalues $$ \lvert \lambda_i \rvert \leq 1 $$. This addresses the gradient exploding problem usually seen with traditional RNNs.


Initially, popular linear RNN architectures constrained the matrix $$A$$ to being diagonal and with nonnegative eigenvalues. Some examples are Mamba and Gated Linear Attention. It turns out these choices limit the expressivity of the model:

**Theorem 1:** (from  <d-cite key="grazzi2025unlockingstatetrackinglinearrnns"></d-cite>) For a finite precision LRNN with finitely many layers, if the eigenvalues of each $$A(x)$$ is nonnegative for all x, then it cannot can solve parity for arbitrary input lengths. In other words, it cannot recognize the language $$(11)^*$$

*Proof idea:* Let's consider a single layer LRNN, as multiple layers can be handled by induction. For simplicity, we will assume all arithmetic operations will be handled in $$\mathbb{R},$$ with infinite precision, and then the cast the result into finite precision $$\mathbb{D}$$, which is a finite subset of $$\mathbb{R}.$$ The closed form expression for the hidden state $$H_k$$ is 

$$H_k = \left( \prod^k_{i=1} A(x_i)\right)H_0+ \sum^k_{i=1}\left( \prod^k_{j=i+1}A(x_j) \right)B(x_i)$$

where we set $$\prod^k_{j=k+1}A(x_j) = I$$. On an input sequence $$x_1,...,x_k=(1)^k$$, we get 

$$H_k = A(1)^k H_0 + \sum^{k}_{i=1}A(1)^{i-1}B(1)$$

Now write $$A(1)=PJP^{-1}$$ in its Jordan canonical form and consider its largest eigenvalue $$\lambda_1$$ in $$J$$. After analyzing the different cases $$\lambda_1 \in [0,1), \lambda_1 = 1$$, and $$\lambda_1 >1$$,  there exists $$\tau$$ such that for all $$k \geq \tau$$ and a fixed matrix $$M,$$ the resulting matrix product becomes constant after taking finite precision: $$cast(A(1)^kM)=\hat M$$. Thus for $$k$$ large enough, $$cast(H_k) = cast(H_{k+1})$$ and the LRNN cannot distinguish between $$(1)^k$$ and$$(1)^{k+1}$$.

**Theorem 2:** (from  <d-cite key="grazzi2025unlockingstatetrackinglinearrnns"></d-cite>): For a finite precision LRNN with finitely many layers, if the eigenvalues of each $$A(x)$$ is real for all $$x$$, then it cannot can solve modular counting $$\mod m$$, where $$m$$ is not a power of $$2$$, for arbitrary input lengths. In other words, it cannot recognize the language $$(1^m)^*$$. 
Note that for a real valued triangular matrix, all of its eigenvalues are real, so a LRNN with triangular state matrix cannot solve modular counting. This motivates the need for general, non-triangular state matrices.

*Proof idea:* It's very similar to the proof of theorem 1, although with negative eigenvalues, a single layer LRNN will have its hidden state alternate between $$2$$ values, for sequence length $$k$$ large enough. For $$L$$ layers, the function $$k \mapsto \hat H_k$$ will be periodic with period $$2^L$$, so if $$m \neq 2^n$$ for some $$n$$, then the LRNN cannot recognize modular counting.

Architectures that allow for negative eigenvalues and non-triangular matrices include modifications of DeltaNet and DeltaProduct, as well as RWKV v7. In DeltaProduct, the state-transition matrix $$A(x_{i})$$ is defined as the product of $$n_h$$ generalized Householder matrices: $$A(x_{i}) = \prod_{j=1}^{n_{h}}(I - \beta_{i,j}k_{i,j}k_{i,j}^{\top}).$$ To allow for negative eigenvalues, they replace $\beta_{i,j}$ with $2\beta_{i,j}$, where $\beta_{i,j}\in [0,1]$ is an input dependent gating factor. This construction enables a rank $$n_h$$​ update to the hidden state at each step. Since a $$k\times k$$ permutation matrix can be formed by a product of $$k-1$$ GH matrices, a single layer DeltaProduct model with $$n_h=k-1$$ and $$n=k$$ can solve the word problem of the permutation group on $$k$$ elements (see theorem 3 in  <d-cite key="grazzi2025unlockingstatetrackinglinearrnns"></d-cite>): simply choose generalized Householder matrices such that $$A(x_i)\in \mathbb{R}^{k \times k}$$ is the permutation matrix representing $$x_i$$ and  $$H_i \in \mathbb{R}^{k\times 1}$$ is the resulting permutation on $$k$$ elements after $$i$$ permutation operations. 


<div class="row mt-3">
    <div class="col-sm-6 mt-3 mt-md-0">
        {% include figure.liquid loading="lazy" path="assets/img/2025-07-04-State-Tracking/A5.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
    <div class="col-sm-6 mt-3 mt-md-0">
        {% include figure.liquid loading="lazy" path="assets/img/2025-07-04-State-Tracking/S5.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    Figures from <d-cite key="siems2025deltaproductimprovingstatetrackinglinear"></d-cite>. Comparing accuracy vs sequence length for DeltaProduct models with $n_h$ GH matrices and 
</div>

In  <d-cite key="siems2025deltaproductimprovingstatetrackinglinear"></d-cite>, they empirically show that the word problem on a permutation group on $$5$$ elements can be solved with a single layer DeltaProduct with $$4$$ GH matrices. Interestingly, the model with $$2$$ $$GH$$ matrices is able to learn permutations from $$A_5$$ by exploiting the structure of the group.


## Conclusion

While a transformer model has a hard time solving compositional tasks like word problems, chain of thought allows it to reuse previous calculations for the next decoding step. On the other hand, linear RNNs, when the state matrix is expressive enough, can solve the word problem without chain of thought. It would be interesting to see if LLMs based on linear RNNs are better at solving compositional tasks in natural language with a shorter chain of thought than current transformer based reasoning LLMs.
